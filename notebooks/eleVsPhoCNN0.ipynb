{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "np.random.seed(1337)  # for reproducibility\n", "import h5py\n", "from keras.models import Sequential\n", "from keras.optimizers import Adam\n", "from keras.initializers import TruncatedNormal, RandomUniform\n", "from keras import regularizers\n", "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n", "from keras.constraints import max_norm\n", "from keras.callbacks import ReduceLROnPlateau"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_curve, auc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_init     = 1.e-3    # Initial learning rate\n", "batch_size  = 100       # Training batch size\n", "train_size  = 100000    # Training size\n", "valid_size  = 10000    # Validation size\n", "test_size   = 10000     # Test size\n", "epochs      = 20       # Number of epochs\n", "doGPU       = True    # Use GPU"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if doGPU:\n", "    import tensorflow as tf\n\n", "    # Create a TensorFlow config object\n", "    config = tf.config.experimental.set_memory_growth\n\n", "    # Set GPU memory growth\n", "    gpus = tf.config.list_physical_devices('GPU')\n", "    if gpus:\n", "        try:\n", "            # Restrict TensorFlow to only use the first GPU\n", "            tf.config.set_visible_devices(gpus[0], 'GPU')\n", "            # Allow memory growth for the GPU\n", "            tf.config.experimental.set_memory_growth(gpus[0], True)\n", "        except RuntimeError as e:\n", "            # Memory growth must be set before GPUs have been initialized\n", "            print(e)"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Load Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img_rows, img_cols, nb_channels = 32, 32, 2\n", "input_dir = 'data'\n", "decays = ['SinglePhotonPt50_IMGCROPS_n249k_RHv1', 'SingleElectronPt50_IMGCROPS_n249k_RHv1']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(decays, start, stop):\n", "    global input_dir\n", "    dsets = [h5py.File('%s/%s.hdf5'%(input_dir,decay)) for decay in decays]\n", "    X = np.concatenate([dset['/X'][start:stop] for dset in dsets])\n", "    y = np.concatenate([dset['/y'][start:stop] for dset in dsets])\n", "    assert len(X) == len(y)\n", "    return X, y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Configure Training/Validation/Test Sets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set range of training set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_start, train_stop = 0, train_size\n", "assert train_stop > train_start\n", "assert (len(decays)*train_size) % batch_size == 0\n", "X_train, y_train = load_data(decays,train_start,train_stop)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set range of validation set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["valid_start, valid_stop = 160000, 160000+valid_size\n", "assert valid_stop  >  valid_start\n", "assert valid_start >= train_stop\n", "X_valid, y_valid = load_data(decays,valid_start,valid_stop)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set range of test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_start, test_stop = 204800, 204800+test_size\n", "assert test_stop  >  test_start\n", "assert test_start >= valid_stop\n", "X_test, y_test = load_data(decays,test_start,test_stop)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["samples_requested = len(decays) * (train_size + valid_size + test_size)\n", "samples_available = len(y_train) + len(y_valid) + len(y_test)\n", "assert samples_requested == samples_available"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Define CNN model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Conv2D(8, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal', input_shape=(img_rows, img_cols, nb_channels)))\n", "model.add(Conv2D(8, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "model.add(Conv2D(32, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n", "model.add(Conv2D(32, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "model.add(Flatten())\n", "model.add(Dense(256, activation='relu', kernel_initializer='TruncatedNormal'))\n", "model.add(Dropout(0.2))\n", "model.add(Dense(128, activation='relu', kernel_initializer='TruncatedNormal'))\n", "model.add(Dropout(0.2))\n", "model.add(Dense(1, activation='sigmoid', kernel_initializer='TruncatedNormal'))\n", "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr_init), metrics=['accuracy'])\n", "model.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1.e-6)\n", "history=model.fit(X_train, y_train,\\\n", "        batch_size=batch_size,\\\n", "        epochs=epochs,\\\n", "        validation_data=(X_valid, y_valid),\\\n", "        callbacks=[reduce_lr],\\\n", "        verbose=1, shuffle=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save Trained neural network"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.save('CNNmodel1.h5')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate on validation set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = model.evaluate(X_valid, y_valid, verbose=1)\n", "print('\\nValidation loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n", "y_pred = model.predict(X_valid)\n", "fpr, tpr, _ = roc_curve(y_valid, y_pred)\n", "roc_auc = auc(fpr, tpr)\n", "print('Validation ROC AUC:', roc_auc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate on test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = model.evaluate(X_test, y_test, verbose=1)\n", "print('\\nTest loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n", "y_pred = model.predict(X_test)\n", "fpr, tpr, _ = roc_curve(y_test, y_pred)\n", "roc_auc = auc(fpr, tpr)\n", "print('Test ROC AUC:', roc_auc)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}